{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO 1 - CGAN para generar un dígito del mnist a pedido del usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importo las librerias neceasarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Dropout, BatchNormalization, Activation, Embedding, Concatenate\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cargo el dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['0','1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "img_size = X_train.shape[1] # tamaño de las imagenes (cuadradas)\n",
    "\n",
    "X_train = np.reshape(X_train, [-1, img_size, img_size, 1])\n",
    "X_train = np.repeat(X_train, 3, axis=-1)  # Convertir a 3 canales\n",
    "X_train = (X_train - 127.5) / 127.5 # los valores se escalan para estar en el rango [-1, 1]\n",
    "y_train = np.expand_dims(y_train, axis=-1) #expando la dimension de y_train \n",
    "\n",
    "X_test = np.reshape(X_test, [-1, img_size, img_size, 1])\n",
    "X_test = np.repeat(X_test, 3, axis=-1)  # Convertir a 3 canales\n",
    "X_test = (X_test - 127.5) / 127.5 # los valores se escalan para estar en el rango [-1, 1]\n",
    "y_test = np.expand_dims(y_test, axis=-1) #expando la dimension de y_test\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ploteo de un número random del dataset junto con su etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "idx = np.random.randint(0,len(X_train))\n",
    "img = image.array_to_img(X_train[idx], scale=True)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Etiqueta: {tags[y_train[idx][0]]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción del generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(n_class, noise_dim, img_size):\n",
    "    # label input\n",
    "    in_label = Input(shape=(1,), name='Label_Input')\n",
    "    # create an embedding layer for all the 10 classes in the form of a vector of size 50 (ver paper)\n",
    "    li = Embedding(n_class, 50, name='Embedding')(in_label)\n",
    "    \n",
    "    img_size_in = img_size // 4\n",
    "\n",
    "    n_nodes = img_size_in * img_size_in\n",
    "    li = Dense(n_nodes, name='Label_Dense')(li)\n",
    "    # reshape the layer\n",
    "    li = Reshape((img_size_in, img_size_in, 1), name='Label_Reshape')(li)\n",
    " \n",
    "    # image generator input\n",
    "    in_lat = Input(shape=(noise_dim,), name='Latent_Input')\n",
    " \n",
    "    n_nodes = 128 * img_size_in * img_size_in\n",
    "    gen = Dense(n_nodes, name='Generator_Dense')(in_lat)\n",
    "    gen = LeakyReLU(negative_slope=0.2, name='Generator_LeakyReLU_1')(gen)\n",
    "    gen = Reshape((img_size_in, img_size_in, 128), name='Generator_Reshape')(gen)\n",
    " \n",
    "    # merge image gen and label input\n",
    "    merge = Concatenate(name='Concatenate')([gen, li])\n",
    " \n",
    "    gen = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='Conv2DTranspose_1')(merge)  # 14x14x128\n",
    "    gen = LeakyReLU(negative_slope=0.2, name='Generator_LeakyReLU_2')(gen)\n",
    " \n",
    "    gen = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', name='Conv2DTranspose_2')(gen)  # 28x28x128\n",
    "    gen = LeakyReLU(negative_slope=0.2, name='Generator_LeakyReLU_3')(gen)\n",
    " \n",
    "    out_layer = Conv2D(3, (8, 8), activation='tanh', padding='same', name='Output_Conv2D')(gen)  # 28x28x3 #acá lo lleva a 32x32x3 porque es el tamaño de las imagenes del mnist \n",
    " \n",
    "    generator = Model([in_lat, in_label], out_layer, name='Generator')\n",
    "    plot_model(generator, to_file='generator_structure.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción del discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(n_class, noise_dim, img_size):\n",
    "    \n",
    "    # label input\n",
    "    in_label = Input(shape=(1,), name='Label_Input')\n",
    "    # This vector of size 50 will be learnt by the discriminator (ver paper)\n",
    "    li = Embedding(n_class, 50, name='Embedding')(in_label)\n",
    "   \n",
    "    n_nodes = img_size * img_size \n",
    "    li = Dense(n_nodes, name='Label_Dense')(li) \n",
    "  \n",
    "    li = Reshape((img_size, img_size, 1), name='Label_Reshape')(li) \n",
    "  \n",
    "    # image input\n",
    "    in_image = Input(shape=(img_size, img_size, 3), name='Image_Input') \n",
    "   \n",
    "    merge = Concatenate(name='Concatenate')([in_image, li]) \n",
    " \n",
    "    # We will combine input label with input image and supply as inputs to the model. \n",
    "    fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same', name='Conv2D_1')(merge) \n",
    "    fe = LeakyReLU(negative_slope=0.2, name='LeakyReLU_1')(fe)\n",
    "   \n",
    "    fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same', name='Conv2D_2')(fe) \n",
    "    fe = LeakyReLU(negative_slope=0.2, name='LeakyReLU_2')(fe)\n",
    "   \n",
    "    fe = Flatten(name='Flatten')(fe) \n",
    "   \n",
    "    fe = Dropout(0.4, name='Dropout')(fe)\n",
    "   \n",
    "    out_layer = Dense(1, activation='sigmoid', name='Output')(fe)\n",
    " \n",
    "    discriminator = Model([in_image, in_label], out_layer, name='Discriminator')\n",
    "    plot_model(discriminator, to_file='discriminator_structure.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(num_samples, noise_dim, g_model, epoch):\n",
    "\n",
    "  fig, axes = plt.subplots(10,num_samples, figsize=(10,20)) \n",
    "  fig.tight_layout()\n",
    "  fig.subplots_adjust(wspace=None, hspace=None)\n",
    "\n",
    "  for l in np.arange(10):\n",
    "    random_noise = tf.random.normal(shape=(num_samples, noise_dim))\n",
    "    label = tf.ones(num_samples)*l\n",
    "    gen_imgs = g_model.predict([random_noise, label])\n",
    "    for j in range(gen_imgs.shape[0]):\n",
    "      img = image.array_to_img(gen_imgs[j], scale=True)\n",
    "      axes[l,j].imshow(img)\n",
    "      axes[l,j].yaxis.set_ticks([])\n",
    "      axes[l,j].xaxis.set_ticks([])\n",
    "\n",
    "      if j ==0:\n",
    "        axes[l,j].set_ylabel(tags[l])\n",
    "\n",
    "  os.makedirs(\"evolution\", exist_ok=True)\n",
    "  plt.savefig(f\"evolution/digits_epoch={epoch}.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de las funciones de pérdida (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss function for Classification between Real and Fake\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    " \n",
    "# Discriminator Loss\n",
    "def discriminator_loss(real, fake):\n",
    "    real_loss = bce_loss(tf.ones_like(real), real) # Calculo la loss para las imagenes reales\n",
    "    fake_loss = bce_loss(tf.zeros_like(fake), fake) # Calculo la loss para las imagenes falsas\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "   \n",
    "# Generator Loss\n",
    "def generator_loss(preds):\n",
    "    return bce_loss(tf.ones_like(preds), preds) # Calculo la loss para el generador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de las métricas para evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la CGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parámetros de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 1 #100 # Cantidad de epocas \n",
    "batch_size = 16 #tamaño del batch hay 60000/16 = 3750 batches\n",
    "noise_dim = 100 # Dimension del ruido\n",
    "learning_rate = 0.0002 \n",
    "n_class = len(tags)  # numero de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construyo el generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optimizer = Adam(learning_rate = learning_rate, beta_1 = 0.5) # Defino el optimizador\n",
    "g_model = build_generator(noise_dim=noise_dim, n_class=n_class, img_size=img_size) # Construyo el generador\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construyo el discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = Adam(learning_rate = learning_rate, beta_1 = 0.5) # Defino el optimizador\n",
    "d_model = build_discriminator(n_class=n_class, noise_dim=noise_dim, img_size=img_size) # Construyo el discriminador\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso de entrenamiento por batches para el conjunto de entrenamiento (se actualizan los pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # Compiles the train_step function into a callable TensorFlow graph\n",
    "def train_step(image_batch, batch_size, noise_dim):\n",
    "\n",
    "    real_images, real_labels = image_batch \n",
    "    # Sample random points in the latent space and concatenate the labels.\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, noise_dim)) # Genero ruido aleatorio\n",
    "    generated_images = g_model([random_latent_vectors, real_labels]) #  Genero imagenes falsas\n",
    " \n",
    "    # Train the discriminator.\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_fake = d_model([generated_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes falsas\n",
    "        pred_real = d_model([real_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes reales\n",
    "         \n",
    "        d_loss = discriminator_loss(pred_real, pred_fake) # Calculo la loss del discriminador\n",
    "    \n",
    "    \n",
    "    all_real_labels = [] # Guardo las etiquetas reales\n",
    "    all_pred_labels = []  # Guardo las etiquetas predichas\n",
    "    \n",
    "    # Extender las listas con las etiquetas correspondientes\n",
    "    for i in range(batch_size):\n",
    "        all_real_labels.append(1)  # Etiqueta real para imágenes reales\n",
    "        all_pred_labels.append(pred_real[i])  # Etiqueta predicha para imágenes reales\n",
    "    for i in range(batch_size):\n",
    "        all_real_labels.append(0)  # Etiqueta real para imágenes falsas\n",
    "        all_pred_labels.append(pred_fake[i])  # Etiqueta predicha para imágenes falsas\n",
    "            \n",
    "       \n",
    "    grads = tape.gradient(d_loss, d_model.trainable_variables) # Calculo los gradientes\n",
    "    d_optimizer.apply_gradients(zip(grads, d_model.trainable_variables)) # Aplico los gradientes al optimizador del discriminador\n",
    " \n",
    "    #-----------------------------------------------------------------#\n",
    "     \n",
    "    # Sample random points in the latent space.\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, noise_dim)) # Genero ruido aleatorio\n",
    "    \n",
    "    # Train the generator\n",
    "    with tf.GradientTape() as tape: \n",
    "        fake_images = g_model([random_latent_vectors, real_labels]) # Genero imagenes falsas\n",
    "        predictions = d_model([fake_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes falsas\n",
    "        g_loss = generator_loss(predictions) # Calculo la loss del generador\n",
    "     \n",
    "    grads = tape.gradient(g_loss, g_model.trainable_variables) # Calculo los gradientes\n",
    "    g_optimizer.apply_gradients(zip(grads, g_model.trainable_variables)) # Aplico los gradientes al optimizador del generador\n",
    "\n",
    "    return d_loss, g_loss, all_real_labels, all_pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso de entrenamiento por batches para el conjunto de entrenamiento (no se actualizan los pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # Compiles the train_step function into a callable TensorFlow graph\n",
    "def test_step(image_batch, batch_size, noise_dim):\n",
    "\n",
    "    real_images, real_labels = image_batch \n",
    "    # Sample random points in the latent space and concatenate the labels.\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, noise_dim)) # Genero ruido aleatorio\n",
    "    generated_images = g_model([random_latent_vectors, real_labels]) #  Genero imagenes falsas\n",
    "\n",
    "    pred_fake = d_model([generated_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes falsas\n",
    "    pred_real = d_model([real_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes reales\n",
    "        \n",
    "    d_loss = discriminator_loss(pred_real, pred_fake) # Calculo la loss del discriminador\n",
    "    \n",
    "    all_real_labels = [] # Guardo las etiquetas reales\n",
    "    all_pred_labels = []  # Guardo las etiquetas predichas\n",
    "    \n",
    "    # Extender las listas con las etiquetas correspondientes\n",
    "    for i in range(batch_size):\n",
    "        all_real_labels.append(1)  # Etiqueta real para imágenes reales\n",
    "        all_pred_labels.append(pred_real[i])  # Etiqueta predicha para imágenes reales\n",
    "    for i in range(batch_size):\n",
    "        all_real_labels.append(0)  # Etiqueta real para imágenes falsas\n",
    "        all_pred_labels.append(pred_fake[i])  # Etiqueta predicha para imágenes falsas\n",
    "\n",
    "    #-----------------------------------------------------------------#\n",
    "     \n",
    "    # Sample random points in the latent space.\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, noise_dim)) # Genero ruido aleatorio\n",
    "    \n",
    "    fake_images = g_model([random_latent_vectors, real_labels]) # Genero imagenes falsas\n",
    "    predictions = d_model([fake_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes falsas\n",
    "    g_loss = generator_loss(predictions) # Calculo la loss del generador\n",
    "\n",
    "    return d_loss, g_loss, all_real_labels, all_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, test_dataset, epoch_count, batch_size):\n",
    "\n",
    "    d_loss_list_epoch_train = []\n",
    "    g_loss_list_epoch_train = []\n",
    "    d_loss_list_epoch_test = []\n",
    "    g_loss_list_epoch_test = []\n",
    "    precision_list_train = []\n",
    "    recall_list_train = []\n",
    "    f1_list_train = []\n",
    "    accuracy_list_train = []\n",
    "    precision_list_test = []\n",
    "    recall_list_test = []\n",
    "    f1_list_test = []\n",
    "    accuracy_list_test = []\n",
    "\n",
    " \n",
    "    for epoch in range(epoch_count):\n",
    "        print('Epoch: ', epoch+1)\n",
    "        d_loss_list_batch_train = []\n",
    "        g_loss_list_batch_train = []\n",
    "        d_loss_list_batch_test = []\n",
    "        g_loss_list_batch_test = []\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        itern = 0\n",
    "        for image_batch in tqdm(train_dataset, desc=f\"Train - Número de batch: \"): # Itero sobre todos los batches\n",
    "            \n",
    "            d_loss_train, g_loss_train, all_real_labels_train, all_pred_labels_train = train_step(image_batch, batch_size, noise_dim) # Entreno el modelo\n",
    "\n",
    "            d_loss_list_batch_train.append(d_loss_train) # Guardo la loss del discriminador\n",
    "            g_loss_list_batch_train.append(g_loss_train) # Guardo la loss del generador\n",
    "            all_real_labels_train = np.array(all_real_labels_train)\n",
    "            all_pred_labels_train = np.array(all_pred_labels_train)\n",
    "            all_pred_labels_train = np.array([i[0] for i in all_pred_labels_train]).reshape((-1,))\n",
    "            \n",
    "            #print(\"all_pred_labels: \", all_pred_labels)\n",
    "            #print(\"all_real_labels: \", all_real_labels)\n",
    "            # print(\"Tipo de all_real_labels:\", type(all_real_labels))\n",
    "            # print(\"Tipo de all_pred_labels_flat:\", type(all_pred_labels))\n",
    "            # print(\"Tipo de all_real_labels:\", all_real_labels.shape)\n",
    "            # print(\"Tipo de all_pred_labels_flat:\", all_pred_labels.shape)\n",
    "            #print(\"Tipo de all_pred_labels_flat:\", type(all_pred_labels))\n",
    "\n",
    "            precision_train, recall_train, f1_train, accuracy_train = calculate_metrics(np.round(all_real_labels_train), np.round(all_pred_labels_train))\n",
    "            precision_list_train.append(precision_train)\n",
    "            recall_list_train.append(recall_train)\n",
    "            f1_list_train.append(f1_train)\n",
    "            accuracy_list_train.append(accuracy_train)\n",
    "\n",
    "            itern=itern+1 \n",
    "\n",
    "        itern = 0\n",
    "        for image_batch in tqdm(test_dataset, desc=f\"Test - Número de batch: \"): # Itero sobre todos los batches\n",
    "            \n",
    "            d_loss_test, g_loss_test, all_real_labels_test, all_pred_labels_test = test_step(image_batch, batch_size, noise_dim) # Entreno el modelo\n",
    "            all_real_labels_test = np.array(all_real_labels_test)\n",
    "            all_pred_labels_test = np.array(all_pred_labels_test)\n",
    "            all_pred_labels_test = np.array([i[0] for i in all_pred_labels_test]).reshape((-1,))\n",
    "            #print(\"all_pred_labels_test: \", all_pred_labels_test)\n",
    "            #print(\"all_real_labels_test: \", all_real_labels_test)\n",
    "            # print(\"Tipo de all_real_labels:\", type(all_real_labels))\n",
    "            # print(\"Tipo de all_pred_labels_flat:\", type(all_pred_labels))\n",
    "            # print(\"Tipo de all_real_labels:\", all_real_labels.shape)\n",
    "            # print(\"Tipo de all_pred_labels_flat:\", all_pred_labels.shape)\n",
    "            #print(\"Tipo de all_pred_labels_flat:\", type(all_pred_labels_test))\n",
    "\n",
    "            d_loss_list_batch_test.append(d_loss_test) # Guardo la loss del discriminador\n",
    "            g_loss_list_batch_test.append(g_loss_test) # Guardo la loss del generador\n",
    "\n",
    "            precision_test, recall_test, f1_test, accuracy_test = calculate_metrics(np.round(all_real_labels_test), np.round(all_pred_labels_test))\n",
    "            precision_list_test.append(precision_test)\n",
    "            recall_list_test.append(recall_test)\n",
    "            f1_list_test.append(f1_test)\n",
    "            accuracy_list_test.append(accuracy_test)\n",
    "\n",
    "            itern=itern+1 \n",
    "        \n",
    "        d_loss_list_epoch_train.append(np.mean(d_loss_list_batch_train))\n",
    "        g_loss_list_epoch_train.append(np.mean(g_loss_list_batch_train))\n",
    "        d_loss_list_epoch_test.append(np.mean(d_loss_list_batch_test))\n",
    "        g_loss_list_epoch_test.append(np.mean(g_loss_list_batch_test))\n",
    "        \n",
    "        print (f'Train - Época: {epoch+1} -- Generator Loss: {np.mean(g_loss_list_batch_train)}, Discriminator Loss: {np.mean(d_loss_list_batch_train)}')\n",
    "        print (f'Test - Época: {epoch+1} -- Generator Loss: {np.mean(g_loss_list_batch_test)}, Discriminator Loss: {np.mean(d_loss_list_batch_test)}\\n')\n",
    "        print (f'Tomó {time.time()-start} segundos. \\n\\n')\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            show_samples(4, n_class, g_model, epoch)\n",
    "\n",
    "    return d_loss_list_epoch_train, g_loss_list_epoch_train, d_loss_list_epoch_test, g_loss_list_epoch_test, precision_list_train, recall_list_train, f1_list_train, accuracy_list_train, precision_list_test, recall_list_test, f1_list_test, accuracy_list_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)) # Se crea un dataset con los datos de entrenamiento\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size) # Se mezclan los datos del dataset cada 1000 y se agrupan en batches de a 16\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)) # Se crea un dataset con los datos de test\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1000).batch(batch_size) # Se mezclan los datos del dataset cada 1000 y se agrupan en batches de a 16\n",
    "\n",
    "d_loss_list_epoch_train, g_loss_list_epoch_train, d_loss_list_epoch_test, g_loss_list_epoch_test, precision_list_train, recall_list_train, f1_list_train, accuracy_list_train, precision_list_test, recall_list_test, f1_list_test, accuracy_list_test  = train(train_dataset, test_dataset, epoch_count, batch_size)\n",
    "\n",
    "g_model.save(\"gmodel_mnist_v1.keras\")\n",
    "d_model.save(\"dmodel_mnist_v1.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficas de la pérdida para el generador y el discriminador en función de las épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "epochs = np.arange(1, epoch_count+1)\n",
    "plt.plot(epochs, g_loss_list_epoch_train, label='Generator Loss - train')\n",
    "plt.plot(epochs, d_loss_list_epoch_train, label='Discriminator Loss - train')\n",
    "plt.plot(epochs, g_loss_list_epoch_test, label='Generator Loss - test')\n",
    "plt.plot(epochs, d_loss_list_epoch_test, label='Discriminator Loss - test')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de un dígito pedido por el usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo generador\n",
    "g_model = load_model('gmodel_mnist_v1.keras')\n",
    "\n",
    "# Preparar la etiqueta para el número 7\n",
    "numero_a_generar = 7\n",
    "\n",
    "label = np.expand_dims(numero_a_generar, axis=-1) #expando la dimension de y_train para que quede analogo al ejemplo del cifar10\n",
    "\n",
    "# Generar ruido aleatorio\n",
    "noise = np.random.normal(size=(1, noise_dim))\n",
    "\n",
    "# Generar imagen falsa \n",
    "generated_image = g_model([noise, label]) #  Genero imagenes falsas\n",
    "#generated_image = g_model.predict([noise, label])\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "print(generated_image.shape)\n",
    "img = image.array_to_img(generated_image[0], scale=True)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"{numero_a_generar}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
