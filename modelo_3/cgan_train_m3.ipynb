{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO 3 - CGAN para generar un dígito del mnist a pedido del usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importo las librerias neceasarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Dropout, BatchNormalization, Activation, Embedding, Concatenate\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(len(devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cargo el dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 3)\n",
      "(60000, 1)\n",
      "(10000, 28, 28, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "tags = ['0','1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "img_size = X_train.shape[1] # tamaño de las imagenes (cuadradas)\n",
    "\n",
    "X_train = np.reshape(X_train, [-1, img_size, img_size, 1])\n",
    "X_train = np.repeat(X_train, 3, axis=-1)  # Convertir a 3 canales\n",
    "X_train = (X_train - 127.5) / 127.5 # los valores se escalan para estar en el rango [-1, 1]\n",
    "y_train = np.expand_dims(y_train, axis=-1) #expando la dimension de y_train \n",
    "\n",
    "X_test = np.reshape(X_test, [-1, img_size, img_size, 1])\n",
    "X_test = np.repeat(X_test, 3, axis=-1)  # Convertir a 3 canales\n",
    "X_test = (X_test - 127.5) / 127.5 # los valores se escalan para estar en el rango [-1, 1]\n",
    "y_test = np.expand_dims(y_test, axis=-1) #expando la dimension de y_test\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ploteo de un número random del dataset junto con su etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "idx = np.random.randint(0,len(X_train))\n",
    "img = image.array_to_img(X_train[idx], scale=True)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Etiqueta: {tags[y_train[idx][0]]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción del generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(n_class, noise_dim, img_size):\n",
    "    # label input\n",
    "    in_label = Input(shape=(1,), name='Label_Input')\n",
    "    # create an embedding layer for all the 10 classes in the form of a vector of size 50 (ver paper)\n",
    "    li = Embedding(n_class, 50, name='Embedding')(in_label)\n",
    "    \n",
    "    img_size_in = img_size // 4\n",
    "\n",
    "    n_nodes = img_size_in * img_size_in \n",
    "    li = Dense(n_nodes, name='Label_Dense')(li) # Capa densa \n",
    "    li = Reshape((img_size_in, img_size_in, 1), name='Label_Reshape')(li) # Cambio la forma de la capa\n",
    " \n",
    "    # image generator input\n",
    "    in_lat = Input(shape=(noise_dim,), name='Latent_Input') # Capa de entrada de ruido\n",
    "    n_nodes = 256 * img_size_in * img_size_in \n",
    "    gen = Dense(n_nodes, name='Generator_Dense')(in_lat) # Capa densa\n",
    "    gen = Reshape((img_size_in, img_size_in, 256), name='Generator_Reshape')(gen) # Cambio la forma de la capa\n",
    "    #---------------------------------------------------------------------------------#\n",
    "    merge = Concatenate(name='Concatenate')([gen, li]) # Concateno las capas\n",
    "    #---------------------------------------------------------------------------------#\n",
    "    gen = Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', name='Conv2DTranspose_1')(merge)  # 14x14x256\n",
    "    gen = BatchNormalization(name='Generator_BatchNormalization_1')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2, name='Generator_LeakyReLU_1')(gen)\n",
    "    #---------------------------------------------------------------------------------#\n",
    "    gen = Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', name='Conv2DTranspose_2')(gen)  # 28x28x128\n",
    "    gen = BatchNormalization(name='Generator_BatchNormalization_2')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2, name='Generator_LeakyReLU_2')(gen)\n",
    "    #---------------------------------------------------------------------------------# \n",
    "    out_layer = Conv2DTranspose(3, (5, 5), strides=(1,1), activation='tanh', padding='same', name='Output_Conv2DTranspose_4')(gen)  # 28x28x3 \n",
    "    #---------------------------------------------------------------------------------# \n",
    "\n",
    "    generator = Model([in_lat, in_label], out_layer, name='Generator')\n",
    "    plot_model(generator, to_file='/content/drive/MyDrive/Lembo/generator_structure_m3.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción del discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(n_class, noise_dim, img_size):\n",
    "    \n",
    "    # label input\n",
    "    in_label = Input(shape=(1,), name='Label_Input')\n",
    "    # This vector of size 50 will be learnt by the discriminator (ver paper)\n",
    "    li = Embedding(n_class, 50, name='Embedding')(in_label)\n",
    "    \n",
    "    n_nodes = img_size * img_size \n",
    "    li = Dense(n_nodes, name='Label_Dense')(li)  \n",
    "    li = Reshape((img_size, img_size, 1), name='Label_Reshape')(li) \n",
    "  \n",
    "    # image input\n",
    "    in_image = Input(shape=(img_size, img_size, 3), name='Image_Input') \n",
    "    in_image = Dropout(0.75, name='Dropout')(in_image)\n",
    "    merge = Concatenate(name='Concatenate')([in_image, li]) \n",
    " \n",
    "    # We will combine input label with input image and supply as inputs to the model. \n",
    "    #---------------------------------------------------------------------------------#\n",
    "    fe = Conv2D(32, (5, 5), strides=(2, 2), padding='same', name='Conv2D_1')(merge) # 14x14x32\n",
    "    fe = LeakyReLU(alpha=0.2, name='LeakyReLU_1')(fe)\n",
    "    #---------------------------------------------------------------------------------#\n",
    "    fe = Conv2D(64, (5, 5), strides=(2, 2), padding='same', name='Conv2D_2')(fe) # 7x7x64\n",
    "    fe = BatchNormalization(name='Generator_BatchNormalization_1')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2, name='LeakyReLU_2')(fe)\n",
    "    #---------------------------------------------------------------------------------#\n",
    "    fe = Conv2D(128, (5, 5), strides=(1, 1), padding='same', name='Conv2D_3')(fe) # 7x7x128\n",
    "    fe = BatchNormalization(name='Generator_BatchNormalization_2')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2, name='LeakyReLU_3')(fe)\n",
    "    #---------------------------------------------------------------------------------#\n",
    "    fe = Conv2D(256, (5, 5), strides=(1, 1), padding='same', name='Conv2D_5')(fe) # 7x7x256\n",
    "    #---------------------------------------------------------------------------------#\n",
    "    fe = Flatten(name='Flatten')(fe) \n",
    "    out_layer = Dense(1, activation='sigmoid', name='Output')(fe)\n",
    "    #---------------------------------------------------------------------------------#\n",
    "\n",
    "    discriminator = Model([in_image, in_label], out_layer, name='Discriminator')\n",
    "    plot_model(discriminator, to_file='/content/drive/MyDrive/Lembo/discriminator_structure_m3.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función para visualizar el entrenamiento del generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(num_samples, noise_dim, g_model, epoch):            \n",
    "  \n",
    "  fig, axes = plt.subplots(10,num_samples, figsize=(10,20)) \n",
    "  fig.tight_layout()\n",
    "  fig.subplots_adjust(wspace=None, hspace=None)\n",
    "\n",
    "  for l in np.arange(10):\n",
    "    random_noise = tf.random.normal(shape=(num_samples, noise_dim))\n",
    "    label = tf.ones(num_samples)*l\n",
    "    gen_imgs = g_model.predict([random_noise, label])\n",
    "    for j in range(gen_imgs.shape[0]):\n",
    "      img = image.array_to_img(gen_imgs[j], scale=True)\n",
    "      axes[l,j].imshow(img)\n",
    "      axes[l,j].yaxis.set_ticks([])\n",
    "      axes[l,j].xaxis.set_ticks([])\n",
    "\n",
    "      if j ==0:\n",
    "        axes[l,j].set_ylabel(tags[l])\n",
    "\n",
    "  os.makedirs(\"/content/drive/MyDrive/Lembo/evolution\", exist_ok=True)\n",
    "  plt.savefig(f\"/content/drive/MyDrive/Lembo/evolution/digits_epoch={epoch}.png\", bbox_inches='tight')\n",
    "  plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de las funciones de pérdida (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss function for Classification between Real and Fake\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    " \n",
    "# Discriminator Loss\n",
    "def discriminator_loss(real, fake):\n",
    "    real_loss = bce_loss(tf.ones_like(real), real) # Calculo la loss para las imagenes reales\n",
    "    fake_loss = bce_loss(tf.zeros_like(fake), fake) # Calculo la loss para las imagenes falsas\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "   \n",
    "# Generator Loss\n",
    "def generator_loss(preds):\n",
    "    return bce_loss(tf.ones_like(preds), preds) # Calculo la loss para el generador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de las métricas para evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la CGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parámetros de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 20 # Cantidad de epocas \n",
    "batch_size = 16 #tamaño del batch hay 60000/16 = 3750 batches\n",
    "noise_dim = 100 # Dimension del ruido\n",
    "lr = 0.0002 # Learning rate\n",
    "n_class = len(tags)  # numero de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construyo el generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optimizer = RMSprop(learning_rate=lr)\n",
    "g_model = build_generator(noise_dim=noise_dim, n_class=n_class, img_size=img_size) # Construyo el generador\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construyo el discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = RMSprop(learning_rate=lr)\n",
    "d_model = build_discriminator(n_class=n_class, noise_dim=noise_dim, img_size=img_size) # Construyo el discriminador\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso de entrenamiento por batches para el conjunto de entrenamiento (se actualizan los pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # Compiles the train_step function into a callable TensorFlow graph\n",
    "def train_step(image_batch, batch_size, noise_dim):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        \n",
    "        real_images, real_labels = image_batch \n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        noise = tf.random.normal(shape=(batch_size, noise_dim)) # Genero ruido aleatorio\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_images = g_model([noise, real_labels], training = True) #  Genero imagenes falsas        \n",
    "            pred_real = d_model([real_images, real_labels], training = True) # Obtengo las predicciones del discriminador para las imagenes reales\n",
    "            pred_fake = d_model([generated_images, real_labels], training = True) # Obtengo las predicciones del discriminador para las imagenes falsas\n",
    "            \n",
    "            g_loss = generator_loss(pred_fake) # Calculo la loss del generador\n",
    "            d_loss = discriminator_loss(pred_real, pred_fake) # Calculo la loss del discriminador\n",
    "\n",
    "        grads_g = gen_tape.gradient(g_loss, g_model.trainable_variables) # Calculo los gradientes\n",
    "        grads_d = disc_tape.gradient(d_loss, d_model.trainable_variables) # Calculo los gradientes\n",
    "        \n",
    "        g_optimizer.apply_gradients(zip(grads_g, g_model.trainable_variables)) # Aplico los gradientes al optimizador del generador\n",
    "        d_optimizer.apply_gradients(zip(grads_d, d_model.trainable_variables)) # Aplico los gradientes al optimizador del discriminador\n",
    "\n",
    "        all_real_labels = [] # Guardo las etiquetas reales\n",
    "        all_pred_labels = []  # Guardo las etiquetas predichas\n",
    "        \n",
    "        # Extender las listas con las etiquetas correspondientes\n",
    "        for i in range(batch_size):\n",
    "            all_real_labels.append(1)  # Etiqueta real para imágenes reales\n",
    "            all_pred_labels.append(pred_real[i])  # Etiqueta predicha para imágenes reales\n",
    "        for i in range(batch_size):\n",
    "            all_real_labels.append(0)  # Etiqueta real para imágenes falsas\n",
    "            all_pred_labels.append(pred_fake[i])  # Etiqueta predicha para imágenes falsas\n",
    "                \n",
    "    return d_loss, g_loss, all_real_labels, all_pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paso de entrenamiento por batches para el conjunto de entrenamiento (no se actualizan los pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # Compiles the train_step function into a callable TensorFlow graph\n",
    "def test_step(image_batch, batch_size, noise_dim):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "\n",
    "        real_images, real_labels = image_batch \n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, noise_dim)) # Genero ruido aleatorio\n",
    "        generated_images = g_model([random_latent_vectors, real_labels]) #  Genero imagenes falsas\n",
    "\n",
    "        pred_fake = d_model([generated_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes falsas\n",
    "        pred_real = d_model([real_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes reales\n",
    "            \n",
    "        d_loss = discriminator_loss(pred_real, pred_fake) # Calculo la loss del discriminador\n",
    "        \n",
    "        all_real_labels = [] # Guardo las etiquetas reales\n",
    "        all_pred_labels = []  # Guardo las etiquetas predichas\n",
    "        \n",
    "        # Extender las listas con las etiquetas correspondientes\n",
    "        for i in range(batch_size):\n",
    "            all_real_labels.append(1)  # Etiqueta real para imágenes reales\n",
    "            all_pred_labels.append(pred_real[i])  # Etiqueta predicha para imágenes reales\n",
    "        for i in range(batch_size):\n",
    "            all_real_labels.append(0)  # Etiqueta real para imágenes falsas\n",
    "            all_pred_labels.append(pred_fake[i])  # Etiqueta predicha para imágenes falsas\n",
    "\n",
    "        #-----------------------------------------------------------------#\n",
    "        \n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, noise_dim)) # Genero ruido aleatorio\n",
    "        \n",
    "        fake_images = g_model([random_latent_vectors, real_labels]) # Genero imagenes falsas\n",
    "        predictions = d_model([fake_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes falsas\n",
    "        g_loss = generator_loss(predictions) # Calculo la loss del generador\n",
    "\n",
    "    return d_loss, g_loss, all_real_labels, all_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, test_dataset, epoch_count, batch_size):\n",
    "\n",
    "    d_loss_list_epoch_train = []\n",
    "    g_loss_list_epoch_train = []\n",
    "    d_loss_list_epoch_test = []\n",
    "    g_loss_list_epoch_test = []\n",
    "    precision_list_epoch_train = []\n",
    "    recall_list_epoch_train = []\n",
    "    f1_list_epoch_train = []\n",
    "    accuracy_list_epoch_train = []\n",
    "    precision_list_epoch_test = []\n",
    "    recall_list_epoch_test = []\n",
    "    f1_list_epoch_test = []\n",
    "    accuracy_list_epoch_test = []\n",
    "    d_loss_list_itern_train = []\n",
    "    g_loss_list_itern_train = []\n",
    "    d_loss_list_itern_test = []\n",
    "    g_loss_list_itern_test = []\n",
    "    iteration_train = []\n",
    "    iteration_test = []\n",
    " \n",
    "    for epoch in range(epoch_count):\n",
    "        print('Epoch: ', epoch+1)\n",
    "        d_loss_list_batch_train_aux = []\n",
    "        g_loss_list_batch_train_aux = []\n",
    "        d_loss_list_batch_test_aux = []\n",
    "        g_loss_list_batch_test_aux = []\n",
    "        precision_list_batch_train = []\n",
    "        recall_list_batch_train = []\n",
    "        f1_list_batch_train = []\n",
    "        accuracy_list_batch_train = []\n",
    "        precision_list_batch_test = []\n",
    "        recall_list_batch_test = []\n",
    "        f1_list_batch_test = []\n",
    "        accuracy_list_batch_test = []\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        itern = 0\n",
    "        for image_batch in tqdm(train_dataset, desc=f\"Train - batch/batches: \"): # Itero sobre todos los batches para el conjunto de entrenamiento\n",
    "            \n",
    "            d_loss_train, g_loss_train, all_real_labels_train, all_pred_labels_train = train_step(image_batch, batch_size, noise_dim) # Entreno el modelo\n",
    "\n",
    "            d_loss_list_batch_train_aux.append(d_loss_train) # Guardo la loss del discriminador\n",
    "            g_loss_list_batch_train_aux.append(g_loss_train) # Guardo la loss del generador\n",
    "            d_loss_list_itern_train.append(d_loss_train) # Guardo la loss del discriminador\n",
    "            g_loss_list_itern_train.append(g_loss_train) # Guardo la loss del generador\n",
    "            iteration_train.append(itern)\n",
    "\n",
    "            all_real_labels_train = np.array(all_real_labels_train)\n",
    "            all_pred_labels_train = np.array(all_pred_labels_train)\n",
    "            all_pred_labels_train = np.array([i[0] for i in all_pred_labels_train]).reshape((-1,))\n",
    "            \n",
    "            precision_train, recall_train, f1_train, accuracy_train = calculate_metrics(np.round(all_real_labels_train), np.round(all_pred_labels_train))\n",
    "            precision_list_batch_train.append(precision_train)\n",
    "            recall_list_batch_train.append(recall_train)\n",
    "            f1_list_batch_train.append(f1_train)\n",
    "            accuracy_list_batch_train.append(accuracy_train)\n",
    "\n",
    "            itern=itern+1 \n",
    "\n",
    "        itern = 0\n",
    "        for image_batch in tqdm(test_dataset, desc=f\"Test - batch/batches: \"): # Itero sobre todos los batches para el conjunto de testeo\n",
    "            \n",
    "            d_loss_test, g_loss_test, all_real_labels_test, all_pred_labels_test = test_step(image_batch, batch_size, noise_dim) # Entreno el modelo\n",
    "\n",
    "            \n",
    "            d_loss_list_batch_test_aux.append(d_loss_test) # Guardo la loss del discriminador\n",
    "            g_loss_list_batch_test_aux.append(g_loss_test) # Guardo la loss del generador\n",
    "            d_loss_list_itern_test.append(d_loss_test) # Guardo la loss del discriminador\n",
    "            g_loss_list_itern_test.append(g_loss_test) # Guardo la loss del generador\n",
    "            iteration_test.append(itern)\n",
    "\n",
    "            all_real_labels_test = np.array(all_real_labels_test)\n",
    "            all_pred_labels_test = np.array(all_pred_labels_test)\n",
    "            all_pred_labels_test = np.array([i[0] for i in all_pred_labels_test]).reshape((-1,))\n",
    "\n",
    "            precision_test, recall_test, f1_test, accuracy_test = calculate_metrics(np.round(all_real_labels_test), np.round(all_pred_labels_test))\n",
    "            precision_list_batch_test.append(precision_test)\n",
    "            recall_list_batch_test.append(recall_test)\n",
    "            f1_list_batch_test.append(f1_test)\n",
    "            accuracy_list_batch_test.append(accuracy_test)\n",
    "\n",
    "            itern=itern+1 \n",
    "        \n",
    "        d_loss_list_epoch_train.append(np.mean(d_loss_list_batch_train_aux))\n",
    "        g_loss_list_epoch_train.append(np.mean(g_loss_list_batch_train_aux))\n",
    "        d_loss_list_epoch_test.append(np.mean(d_loss_list_batch_test_aux))\n",
    "        g_loss_list_epoch_test.append(np.mean(g_loss_list_batch_test_aux))\n",
    "        precision_list_epoch_train.append(np.mean(precision_list_batch_train))\n",
    "        recall_list_epoch_train.append(np.mean(recall_list_batch_train))\n",
    "        f1_list_epoch_train.append(np.mean(f1_list_batch_train))\n",
    "        accuracy_list_epoch_train.append(np.mean(accuracy_list_batch_train))\n",
    "        precision_list_epoch_test.append(np.mean(precision_list_batch_test))\n",
    "        recall_list_epoch_test.append(np.mean(recall_list_batch_test))\n",
    "        f1_list_epoch_test.append(np.mean(f1_list_batch_test))\n",
    "        accuracy_list_epoch_test.append(np.mean(accuracy_list_batch_test))\n",
    "\n",
    "        print (f'Train - Época: {epoch+1} -- Generator Loss: {np.mean(g_loss_list_batch_train_aux)}, Discriminator Loss: {np.mean(d_loss_list_batch_train_aux)}')\n",
    "        print (f'Test - Época: {epoch+1} -- Generator Loss: {np.mean(g_loss_list_batch_test_aux)}, Discriminator Loss: {np.mean(d_loss_list_batch_test_aux)}\\n')\n",
    "        print (f'Tomó {time.time()-start} segundos. \\n\\n')\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            show_samples(4, noise_dim, g_model, epoch)\n",
    "\n",
    "    return iteration_test, iteration_train, d_loss_list_itern_train, g_loss_list_itern_train, d_loss_list_itern_test, g_loss_list_itern_test, d_loss_list_epoch_train, g_loss_list_epoch_train, d_loss_list_epoch_test, g_loss_list_epoch_test, precision_list_epoch_train, recall_list_epoch_train, f1_list_epoch_train, accuracy_list_epoch_train, precision_list_epoch_test, recall_list_epoch_test, f1_list_epoch_test, accuracy_list_epoch_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)) # Se crea un dataset con los datos de entrenamiento\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size) # Se mezclan los datos del dataset cada 1000 y se agrupan en batches de a 16\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)) # Se crea un dataset con los datos de test\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1000).batch(batch_size) # Se mezclan los datos del dataset cada 1000 y se agrupan en batches de a 16\n",
    "\n",
    "iteration_test, iteration_train, d_loss_itern_train, g_loss_itern_train, d_loss_itern_test, g_loss_itern_test, d_loss_train, g_loss_train, d_loss_test, g_loss_test, precision_train, recall_train, f1_train, accuracy_train, precision_test, recall_test, f1_test, accuracy_test  = train(train_dataset, test_dataset, epoch_count, batch_size)\n",
    "epochs = np.arange(1, epoch_count+1)\n",
    "\n",
    "np.savez(\"/content/drive/MyDrive/Lembo/metricas_mnist_m3.npz\", epochs= epochs, iteration_test=iteration_test, iteration_train=iteration_train, d_loss_itern_train=d_loss_itern_train, g_loss_itern_train=g_loss_itern_train, d_loss_itern_test=d_loss_itern_test, g_loss_itern_test=g_loss_itern_test, d_loss_train=d_loss_train, g_loss_train=g_loss_train, d_loss_test=d_loss_test, g_loss_test=g_loss_test, precision_train=precision_train, recall_train=recall_train, f1_train=f1_train, accuracy_train=accuracy_train, precision_test=precision_test, recall_test=recall_test, f1_test=f1_test, accuracy_test=accuracy_test)\n",
    "g_model.save(\"/content/drive/MyDrive/Lembo/gmodel_mnist_m3.keras\")\n",
    "d_model.save(\"/content/drive/MyDrive/Lembo/dmodel_mnist_m3.keras\")\n",
    "g_model.save(\"/content/drive/MyDrive/Lembo/gmodel_mnist_m3.H5\")\n",
    "d_model.save(\"/content/drive/MyDrive/Lembo/dmodel_mnist_m3.H5\")\n",
    "g_model.save(\"/content/drive/MyDrive/Lembo/gmodel_mnist_m3.h5\")\n",
    "d_model.save(\"/content/drive/MyDrive/Lembo/dmodel_mnist_m3.h5\")\n",
    "g_model.save(\"/content/drive/MyDrive/Lembo/gmodel_mnist_m3.tf\")\n",
    "d_model.save(\"/content/drive/MyDrive/Lembo/dmodel_mnist_m3.tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficas de la pérdida para el generador y el discriminador en función del número de iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/content/drive/MyDrive/Lembo/metricas_mnist_m3.npz\")\n",
    "iteration_train = data['iteration_train']\n",
    "iteration_test = data['iteration_test']\n",
    "d_loss_train = data['d_loss_itern_train']\n",
    "g_loss_train = data['g_loss_itern_train']\n",
    "d_loss_test = data['d_loss_itern_test']\n",
    "g_loss_test = data['g_loss_itern_test']\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(iteration_train, g_loss_itern_train, label='Generator Loss - train')\n",
    "plt.plot(iteration_train, d_loss_itern_train, label='Discriminator Loss - train')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(iteration_test, g_loss_itern_test, label='Generator Loss - test')\n",
    "plt.plot(iteration_test, d_loss_itern_test, label='Discriminator Loss - test')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficas de la pérdida para el generador y el discriminador en función de las épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/content/drive/MyDrive/Lembo/metricas_mnist_m3.npz\")\n",
    "epochs = data['epochs']\n",
    "d_loss_train = data['d_loss_train']\n",
    "g_loss_train = data['g_loss_train']\n",
    "d_loss_test = data['d_loss_test']\n",
    "g_loss_test = data['g_loss_test']\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epochs, g_loss_train, label='Generator Loss - train')\n",
    "plt.plot(epochs, d_loss_train, label='Discriminator Loss - train')\n",
    "plt.plot(epochs, g_loss_test, label='Generator Loss - test')\n",
    "plt.plot(epochs, d_loss_test, label='Discriminator Loss - test')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficas de las métricas para el discriminador en función del número de épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/content/drive/MyDrive/Lembo/metricas_mnist_m3.npz\")\n",
    "epochs = data['epochs']\n",
    "precision_train = data['precision_train']\n",
    "recall_train = data['recall_train']\n",
    "f1_train = data['f1_train']\n",
    "accuracy_train = data['accuracy_train']\n",
    "precision_test = data['precision_test']\n",
    "recall_test = data['recall_test']\n",
    "f1_test = data['f1_test']\n",
    "accuracy_test = data['accuracy_test']\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "epochs = np.arange(1, epoch_count + 1)\n",
    "axs[0, 0].plot(epochs, precision_train, label='Train')\n",
    "axs[0, 0].plot(epochs, precision_test, label='Test')\n",
    "axs[0, 0].set_xlabel('Épocas')\n",
    "axs[0, 0].set_ylabel('Precisión')\n",
    "axs[0, 0].legend()\n",
    "axs[0, 0].set_title('Precisión')\n",
    "axs[0, 1].plot(epochs, recall_train, label='Train')\n",
    "axs[0, 1].plot(epochs, recall_test, label='Test')\n",
    "axs[0, 1].set_xlabel('Épocas')\n",
    "axs[0, 1].set_ylabel('Recall')\n",
    "axs[0, 1].legend()\n",
    "axs[0, 1].set_title('Recall')\n",
    "axs[1, 0].plot(epochs, f1_train, label='Train')\n",
    "axs[1, 0].plot(epochs, f1_test, label='Test')\n",
    "axs[1, 0].set_xlabel('Épocas')\n",
    "axs[1, 0].set_ylabel('F1-score')\n",
    "axs[1, 0].legend()\n",
    "axs[1, 0].set_title('F1-score')\n",
    "axs[1, 1].plot(epochs, accuracy_train, label='Train')\n",
    "axs[1, 1].plot(epochs, accuracy_test, label='Test')\n",
    "axs[1, 1].set_xlabel('Épocas')\n",
    "axs[1, 1].set_ylabel('Accuracy')\n",
    "axs[1, 1].legend()\n",
    "axs[1, 1].set_title('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de un dígito pedido por el usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo generador\n",
    "g_model = load_model('/content/drive/MyDrive/Lembo/gmodel_mnist_m3.keras')\n",
    "\n",
    "# Preparar la etiqueta para el número 7\n",
    "numero_a_generar = 7\n",
    "\n",
    "#label = tf.expand_dims(numero_a_generar, axis=-1) #expando la dimension de y_train para que quede analogo al ejemplo del cifar10\n",
    "label = tf.ones(1)*numero_a_generar\n",
    "\n",
    "# Generar ruido aleatorio\n",
    "noise = tf.random.normal(shape=(1, noise_dim))\n",
    "\n",
    "# Generar imagen falsa \n",
    "generated_image = g_model([noise, label]) #  Genero imagenes falsas\n",
    "#generated_image = g_model.predict([noise, label])\n",
    "print(\"Tamaño imagen generada: \", generated_image.shape)\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "img = image.array_to_img(generated_image[0], scale=True)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"{numero_a_generar}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
