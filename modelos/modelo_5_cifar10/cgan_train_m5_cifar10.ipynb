{"cells":[{"cell_type":"markdown","metadata":{"id":"jd_gpgCCHjDP"},"source":["# MODELO 5 - CGAN para generar una imagen del CIFAR-10 a pedido del usuario"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"KMrAL1pKHjDR"},"source":["- Importo las librerias neceasarias"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13672,"status":"ok","timestamp":1718848732147,"user":{"displayName":"Ignacio Lembo Ferrari","userId":"11139872009396123551"},"user_tz":180},"id":"wJ5X3j39HjDR"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.optimizers import RMSprop, Adam\n","from tensorflow.keras.datasets import cifar10\n","from keras.preprocessing import image\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Dropout, BatchNormalization, Embedding, Concatenate\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","import os\n","from tqdm import tqdm\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1718844934869,"user":{"displayName":"Juan Pablo Morales","userId":"00326788642118637469"},"user_tz":180},"id":"PrzEXWPbHjDR","outputId":"2a161714-5186-43c9-f44f-9b922d8c432b"},"outputs":[],"source":["devices = tf.config.list_physical_devices('GPU')\n","print(len(devices))"]},{"cell_type":"markdown","metadata":{"id":"KRcsjgwTHjDS"},"source":["- Cargo el dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1631,"status":"ok","timestamp":1718844936498,"user":{"displayName":"Juan Pablo Morales","userId":"00326788642118637469"},"user_tz":180},"id":"uxTSLdZjHjDS","outputId":"aac26eb4-9656-4930-9e19-c12e17901df4"},"outputs":[],"source":["tags = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n","\n","(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","img_size = X_train.shape[1] # tamaño de las imagenes (cuadradas)\n","\n","X_train = (X_train - 127.5) / 127.5\n","X_test = (X_test - 127.5) / 127.5 # los valores se escalan para estar en el rango [-1, 1]\n","\n","print(img_size)\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Z04Ag0tTHjDS"},"source":["- Ploteo de un número random del dataset junto con su etiqueta"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1718844936498,"user":{"displayName":"Juan Pablo Morales","userId":"00326788642118637469"},"user_tz":180},"id":"UhrEJsi8HjDS","outputId":"95f314d3-d190-441f-b39c-0dc2693d5af5"},"outputs":[],"source":["plt.figure(figsize=(2,2))\n","idx = np.random.randint(0,len(X_train))\n","img = image.array_to_img(X_train[idx], scale=True)\n","plt.imshow(img)\n","plt.axis('off')\n","plt.title(tags[y_train[idx][0]])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"kLP3fgXGHjDS"},"source":["# Construcción del generador"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0offIG2HjDS"},"outputs":[],"source":["def build_generator(n_class, noise_dim, img_size):\n","\n","    in_label = Input(shape=(1,), name='Label_Input')\n","    li = Embedding(n_class, 50, name='Embedding')(in_label) # Capa de embedding para las etiquetas\n","    img_size_in = img_size // 4\n","    n_nodes = img_size_in * img_size_in\n","    li = Dense(n_nodes, name='Label_Dense')(li) # Capa densa para las etiquetas\n","    li = Reshape((img_size_in, img_size_in, 1), name='Label_Reshape')(li) # Cambio la forma de la capa\n","\n","    in_lat = Input(shape=(noise_dim,), name='Latent_Input') # Capa de entrada de ruido\n","    n_nodes = 512 * img_size_in * img_size_in\n","    gen = Dense(n_nodes, name='Generator_Dense')(in_lat) # Capa densa para el ruido\n","    gen = Reshape((img_size_in, img_size_in, 512), name='Generator_Reshape')(gen) # Cambio la forma de la capa\n","    #---------------------------------------------------------------------------------#\n","    merge = Concatenate(name='Concatenate')([gen, li]) # Concateno las capas\n","    #---------------------------------------------------------------------------------#\n","    gen = Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', name='Conv2DTranspose_1')(merge)  # 16x16x512\n","    gen = BatchNormalization(momentum=0.8, name='Generator_BatchNormalization_1')(gen)\n","    gen = LeakyReLU(alpha=0.2, name='Generator_LeakyReLU_1')(gen)\n","    #---------------------------------------------------------------------------------#\n","    gen = Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', name='Conv2DTranspose_2')(gen)  # 32x32x256\n","    gen = BatchNormalization(momentum=0.8, name='Generator_BatchNormalization_2')(gen)\n","    gen = LeakyReLU(alpha=0.2, name='Generator_LeakyReLU_2')(gen)\n","    #---------------------------------------------------------------------------------#\n","    gen = Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', name='Conv2DTranspose_3')(gen)  # 32x32x128\n","    gen = BatchNormalization(momentum=0.8, name='Generator_BatchNormalization_3')(gen)\n","    gen = LeakyReLU(alpha=0.2, name='Generator_LeakyReLU_3')(gen)\n","    #---------------------------------------------------------------------------------#\n","    out_layer = Conv2DTranspose(3, (5, 5), strides=(1,1), activation='tanh', padding='same', name='Output_Conv2DTranspose_4')(gen)  # 32x32x3 \n","    #---------------------------------------------------------------------------------#\n","\n","    generator = Model([in_lat, in_label], out_layer, name='Generator')\n","    plot_model(generator, to_file='/content/drive/MyDrive/Lembo/generator_structure_m5_cifar10.png', show_shapes=True, show_layer_names=True)\n","\n","    return generator"]},{"cell_type":"markdown","metadata":{"id":"BOR43pyzHjDS"},"source":["# Construcción del discriminador"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5ZAUbucHjDS"},"outputs":[],"source":["def build_discriminator(n_class, noise_dim, img_size):\n","\n","    in_label = Input(shape=(1,), name='Label_Input')\n","    li = Embedding(n_class, 50, name='Embedding')(in_label)\n","    n_nodes = img_size * img_size\n","    li = Dense(n_nodes, name='Label_Dense')(li)\n","    li = Reshape((img_size, img_size, 1), name='Label_Reshape')(li)\n","\n","    in_image = Input(shape=(img_size, img_size, 3), name='Image_Input')\n","    #in_image = Dropout(0.75, name='Dropout')(in_image)\n","    merge = Concatenate(name='Concatenate')([in_image, li])\n","    #---------------------------------------------------------------------------------#\n","    fe = Conv2D(32, (5, 5), strides=(2, 2), padding='same', name='Conv2D_1')(merge) # 16x16x32\n","    fe = LeakyReLU(alpha=0.2, name='LeakyReLU_1')(fe)\n","    #---------------------------------------------------------------------------------#\n","    fe = Conv2D(64, (5, 5), strides=(2, 2), padding='same', name='Conv2D_2')(fe) # 8x8x64\n","    fe = BatchNormalization(momentum=0.8,name='Generator_BatchNormalization_1')(fe)\n","    fe = LeakyReLU(alpha=0.2, name='LeakyReLU_2')(fe)\n","    #---------------------------------------------------------------------------------#\n","    fe = Conv2D(128, (5, 5), strides=(2, 2), padding='same', name='Conv2D_3')(fe) # 4x4x128\n","    fe = BatchNormalization(momentum=0.8,name='Generator_BatchNormalization_2')(fe)\n","    fe = LeakyReLU(alpha=0.2, name='LeakyReLU_3')(fe)\n","    #---------------------------------------------------------------------------------#\n","    fe = Conv2D(256, (5, 5), strides=(2, 2), padding='same', name='Conv2D_4')(fe) # 4x4x128\n","    fe = BatchNormalization(momentum=0.8, name='Generator_BatchNormalization_3')(fe)\n","    fe = LeakyReLU(alpha=0.2, name='LeakyReLU_4')(fe)\n","    #---------------------------------------------------------------------------------#\n","    fe = Conv2D(512, (5, 5), strides=(2, 2), padding='same', name='Conv2D_5')(fe) # 2x2x256\n","    #---------------------------------------------------------------------------------#\n","    fe = Flatten(name='Flatten')(fe)\n","    fe = Dropout(0.4, name='Dropout')(fe)\n","    out_layer = Dense(1, activation='sigmoid', name='Output_Dense')(fe)\n","    #---------------------------------------------------------------------------------#\n","\n","    discriminator = Model([in_image, in_label], out_layer, name='Discriminator')\n","    plot_model(discriminator, to_file='/content/drive/MyDrive/Lembo/discriminator_structure_m5_cifar10.png', show_shapes=True, show_layer_names=True)\n","\n","    return discriminator"]},{"cell_type":"markdown","metadata":{"id":"sy0BDW2UHjDT"},"source":["# Función para visualizar el entrenamiento del generador"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pi2VS4OeHjDT"},"outputs":[],"source":["def show_samples(num_samples, noise_dim, g_model, epoch):\n","\n","  fig, axes = plt.subplots(10, num_samples, figsize=(10,20))\n","  fig.tight_layout()\n","  fig.subplots_adjust(wspace=None, hspace=None)\n","\n","  for l in np.arange(10):\n","    random_noise = tf.random.normal(shape=(num_samples, noise_dim))\n","    label = tf.ones(num_samples)*l\n","    gen_imgs = g_model.predict([random_noise, label])\n","    for j in range(gen_imgs.shape[0]):\n","      img = image.array_to_img(gen_imgs[j], scale=True)\n","      axes[l,j].imshow(img)\n","      axes[l,j].yaxis.set_ticks([])\n","      axes[l,j].xaxis.set_ticks([])\n","\n","      if j ==0:\n","        axes[l,j].set_ylabel(tags[l])\n","\n","  os.makedirs(\"/content/drive/MyDrive/Lembo/evolution_m5_cifar10\", exist_ok=True)\n","  plt.savefig(f\"/content/drive/MyDrive/Lembo/evolution_m5_cifar10/images_epoch={epoch}.png\", bbox_inches='tight')\n","  plt.close(fig)"]},{"cell_type":"markdown","metadata":{"id":"QsOLl0smHjDT"},"source":["# Definición de las funciones de pérdida (loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3i4eQ7_rHjDT"},"outputs":[],"source":["bce_loss = tf.keras.losses.BinaryCrossentropy()\n","\n","# Discriminator Loss\n","def discriminator_loss(real, fake):\n","    real_loss = bce_loss(tf.ones_like(real), real) # Calculo la loss para las imagenes reales\n","    fake_loss = bce_loss(tf.zeros_like(fake), fake) # Calculo la loss para las imagenes falsas\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","# Generator Loss\n","def generator_loss(preds):\n","    return bce_loss(tf.ones_like(preds), preds) # Calculo la loss para el generador"]},{"cell_type":"markdown","metadata":{"id":"KpC7n6wOHjDT"},"source":["# Definición de las métricas para evaluar el modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfLmTR6FHjDT"},"outputs":[],"source":["def calculate_metrics(y_true, y_pred):\n","    precision = precision_score(y_true, y_pred, zero_division=1)\n","    recall = recall_score(y_true, y_pred, zero_division=1)\n","    f1 = f1_score(y_true, y_pred, zero_division=1)\n","    accuracy = accuracy_score(y_true, y_pred)\n","\n","    return precision, recall, f1, accuracy"]},{"cell_type":"markdown","metadata":{"id":"hsK9-ZycHjDU"},"source":["# Entrenamiento de la CGAN"]},{"cell_type":"markdown","metadata":{"id":"Zf1Jeq4tHjDU"},"source":["- Parámetros de entrenamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P4WUD3bXHjDU"},"outputs":[],"source":["epoch_count = 20 # Cantidad de epocas\n","batch_size = 50 #tamaño del batch hay 60000/16 = 3750 batches\n","noise_dim = 100 # Dimension del ruido\n","lr = 0.0002 # Learning rate\n","n_class = len(tags)  # numero de clases"]},{"cell_type":"markdown","metadata":{"id":"LMWeYYsaHjDU"},"source":["- Construyo el generador"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":777,"status":"ok","timestamp":1718844937270,"user":{"displayName":"Juan Pablo Morales","userId":"00326788642118637469"},"user_tz":180},"id":"x10An-bBHjDU","outputId":"691a32c3-1fc9-49be-9e33-32fd75f99e4c"},"outputs":[],"source":["g_optimizer = Adam(learning_rate=lr, beta_1=0.5) # RMSprop(learning_rate=lr)\n","g_model = build_generator(noise_dim=noise_dim, n_class=n_class, img_size=img_size) # Construyo el generador\n","g_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"eX6dXv-oHjDU"},"source":["- Construyo el discriminador"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1718844937270,"user":{"displayName":"Juan Pablo Morales","userId":"00326788642118637469"},"user_tz":180},"id":"pQqzQ2gPHjDU","outputId":"0592b3d9-b1ce-46c0-9485-461c4e357ddc"},"outputs":[],"source":["d_optimizer = Adam(learning_rate=lr, beta_1=0.5) # RMSprop(learning_rate=lr)\n","d_model = build_discriminator(n_class=n_class, noise_dim=noise_dim, img_size=img_size) # Construyo el discriminador\n","d_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"CxRmvsSUHjDU"},"source":["- Paso de entrenamiento por batches para el conjunto de entrenamiento (se actualizan los pesos)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xdplaio5HjDU"},"outputs":[],"source":["@tf.function # Compiles the train_step function into a callable TensorFlow graph\n","def train_step(image_batch, batch_size, noise_dim):\n","    with tf.device('/device:GPU:0'):\n","\n","        real_images, real_labels = image_batch\n","        noise = tf.random.normal(shape=(batch_size, noise_dim)) # Genero ruido aleatorio\n","\n","        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","            generated_images = g_model([noise, real_labels], training = True) #  Genero imagenes falsas\n","            pred_real = d_model([real_images, real_labels], training = True) # Obtengo las predicciones del discriminador para las imagenes reales\n","            pred_fake = d_model([generated_images, real_labels], training = True) # Obtengo las predicciones del discriminador para las imagenes falsas\n","\n","            g_loss = generator_loss(pred_fake) # Calculo la loss del generador\n","            d_loss = discriminator_loss(pred_real, pred_fake) # Calculo la loss del discriminador\n","\n","        grads_g = gen_tape.gradient(g_loss, g_model.trainable_variables) # Calculo los gradientes\n","        grads_d = disc_tape.gradient(d_loss, d_model.trainable_variables) # Calculo los gradientes\n","\n","        g_optimizer.apply_gradients(zip(grads_g, g_model.trainable_variables)) # Aplico los gradientes al optimizador del generador\n","        d_optimizer.apply_gradients(zip(grads_d, d_model.trainable_variables)) # Aplico los gradientes al optimizador del discriminador\n","\n","        all_real_labels = [] # Guardo las etiquetas reales\n","        all_pred_labels = []  # Guardo las etiquetas predichas\n","\n","        for i in range(batch_size):\n","            all_real_labels.append(1)  # Etiqueta real para imágenes reales\n","            all_pred_labels.append(pred_real[i])  # Etiqueta predicha para imágenes reales\n","        for i in range(batch_size):\n","            all_real_labels.append(0)  # Etiqueta real para imágenes falsas\n","            all_pred_labels.append(pred_fake[i])  # Etiqueta predicha para imágenes falsas\n","\n","    return d_loss, g_loss, all_real_labels, all_pred_labels"]},{"cell_type":"markdown","metadata":{"id":"UfWLPa9DHjDU"},"source":["- Paso de entrenamiento por batches para el conjunto de entrenamiento (no se actualizan los pesos)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2882vOXHjDV"},"outputs":[],"source":["@tf.function # Compiles the test_step function into a callable TensorFlow graph\n","def test_step(image_batch, batch_size, noise_dim):\n","    with tf.device('/device:GPU:0'):\n","\n","        real_images, real_labels = image_batch\n","        noise = tf.random.normal(shape=(batch_size, noise_dim)) # Genero ruido aleatorio\n","        generated_images = g_model([noise, real_labels]) #  Genero imagenes falsas\n","\n","        pred_fake = d_model([generated_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes falsas\n","        pred_real = d_model([real_images, real_labels]) # Obtengo las predicciones del discriminador para las imagenes reales\n","\n","        d_loss = discriminator_loss(pred_real, pred_fake) # Calculo la loss del discriminador\n","        g_loss = generator_loss(pred_fake) # Calculo la loss del generador\n","    \n","        all_real_labels = [] # Guardo las etiquetas reales\n","        all_pred_labels = []  # Guardo las etiquetas predichas\n","\n","        for i in range(batch_size):\n","            all_real_labels.append(1)  # Etiqueta real para imágenes reales\n","            all_pred_labels.append(pred_real[i])  # Etiqueta predicha para imágenes reales\n","        for i in range(batch_size):\n","            all_real_labels.append(0)  # Etiqueta real para imágenes falsas\n","            all_pred_labels.append(pred_fake[i])  # Etiqueta predicha para imágenes falsas\n","\n","    return d_loss, g_loss, all_real_labels, all_pred_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWOc6lguHjDV"},"outputs":[],"source":["def train(train_dataset, test_dataset, epoch_count, batch_size):\n","    \n","    num_train_batches = len(train_dataset)\n","    num_test_batches = len(test_dataset)\n","\n","    d_loss_list_epoch_train = np.zeros(epoch_count)\n","    g_loss_list_epoch_train = np.zeros(epoch_count)\n","    d_loss_list_epoch_test = np.zeros(epoch_count)\n","    g_loss_list_epoch_test = np.zeros(epoch_count)\n","    precision_list_epoch_train = np.zeros(epoch_count)\n","    recall_list_epoch_train = np.zeros(epoch_count)\n","    f1_list_epoch_train = np.zeros(epoch_count)\n","    accuracy_list_epoch_train = np.zeros(epoch_count)\n","    precision_list_epoch_test = np.zeros(epoch_count)\n","    recall_list_epoch_test = np.zeros(epoch_count)\n","    f1_list_epoch_test = np.zeros(epoch_count)\n","    accuracy_list_epoch_test = np.zeros(epoch_count)\n","    d_loss_list_itern_train = np.zeros(epoch_count *num_train_batches)\n","    g_loss_list_itern_train = np.zeros(epoch_count * num_train_batches)\n","    d_loss_list_itern_test = np.zeros(epoch_count * num_test_batches)\n","    g_loss_list_itern_test = np.zeros(epoch_count * num_test_batches)\n","\n","    itern_train_counter = 0\n","    itern_test_counter = 0\n"," \n","    for epoch in range(epoch_count):\n","        print('Epoch: ', epoch+1)\n","\n","        d_loss_list_batch_train_aux = np.zeros(num_train_batches)\n","        g_loss_list_batch_train_aux = np.zeros(num_train_batches)\n","        d_loss_list_batch_test_aux = np.zeros(num_test_batches)\n","        g_loss_list_batch_test_aux = np.zeros(num_test_batches)\n","        precision_list_batch_train = np.zeros(num_train_batches)\n","        recall_list_batch_train = np.zeros(num_train_batches)\n","        f1_list_batch_train = np.zeros(num_train_batches)\n","        accuracy_list_batch_train = np.zeros(num_train_batches)\n","        precision_list_batch_test = np.zeros(num_test_batches)\n","        recall_list_batch_test = np.zeros(num_test_batches)\n","        f1_list_batch_test = np.zeros(num_test_batches)\n","        accuracy_list_batch_test = np.zeros(num_test_batches)\n","\n","        start = time.time()\n","\n","        itern = 0\n","        for image_batch in tqdm(train_dataset, desc=f\"Train - batch/batches: \"): # Itero sobre todos los batches para el conjunto de entrenamiento\n","            \n","            d_loss_train, g_loss_train, all_real_labels_train, all_pred_labels_train = train_step(image_batch, batch_size, noise_dim) # Entreno el modelo\n","\n","            d_loss_list_batch_train_aux[itern] = d_loss_train \n","            g_loss_list_batch_train_aux[itern] = g_loss_train  \n","            d_loss_list_itern_train[itern_train_counter] = d_loss_train  \n","            g_loss_list_itern_train[itern_train_counter] = g_loss_train  \n","            itern_train_counter += 1\n","\n","            all_real_labels_train = np.array(all_real_labels_train)\n","            all_pred_labels_train = np.array(all_pred_labels_train)\n","            all_pred_labels_train = np.array([i[0] for i in all_pred_labels_train]).reshape((-1,)) # Reshapeo para que all_pred_labels_train (tiene la dimensión de la salida del discriminador) tenga la misma forma que all_real_labels_train\n","            \n","            precision_train, recall_train, f1_train, accuracy_train = calculate_metrics(np.round(all_real_labels_train), np.round(all_pred_labels_train))\n","            precision_list_batch_train[itern] = precision_train\n","            recall_list_batch_train[itern] = recall_train\n","            f1_list_batch_train[itern] = f1_train\n","            accuracy_list_batch_train[itern] = accuracy_train\n","            itern=itern+1 \n","\n","        itern = 0\n","        for image_batch in tqdm(test_dataset, desc=f\"Test - batch/batches: \"): # Itero sobre todos los batches para el conjunto de testeo\n","            \n","            d_loss_test, g_loss_test, all_real_labels_test, all_pred_labels_test = test_step(image_batch, batch_size, noise_dim) # Entreno el modelo\n","\n","            d_loss_list_batch_test_aux[itern] = d_loss_test\n","            g_loss_list_batch_test_aux[itern] = g_loss_test \n","            d_loss_list_itern_test[itern_test_counter] = d_loss_test \n","            g_loss_list_itern_test[itern_test_counter] = g_loss_test \n","            itern_test_counter += 1\n","\n","            all_real_labels_test = np.array(all_real_labels_test)\n","            all_pred_labels_test = np.array(all_pred_labels_test)\n","            all_pred_labels_test = np.array([i[0] for i in all_pred_labels_test]).reshape((-1,))\n","\n","            precision_test, recall_test, f1_test, accuracy_test = calculate_metrics(np.round(all_real_labels_test), np.round(all_pred_labels_test))\n","            precision_list_batch_test[itern] = precision_test\n","            recall_list_batch_test[itern] = recall_test\n","            f1_list_batch_test[itern] = f1_test\n","            accuracy_list_batch_test[itern] = accuracy_test\n","\n","            itern=itern+1 \n","        \n","        d_loss_list_epoch_train[epoch] = np.mean(d_loss_list_batch_train_aux)\n","        g_loss_list_epoch_train[epoch] = np.mean(g_loss_list_batch_train_aux)\n","        d_loss_list_epoch_test[epoch] = np.mean(d_loss_list_batch_test_aux)\n","        g_loss_list_epoch_test[epoch] = np.mean(g_loss_list_batch_test_aux)\n","        precision_list_epoch_train[epoch] = np.mean(precision_list_batch_train)\n","        recall_list_epoch_train[epoch] = np.mean(recall_list_batch_train)\n","        f1_list_epoch_train[epoch] = np.mean(f1_list_batch_train)\n","        accuracy_list_epoch_train[epoch] = np.mean(accuracy_list_batch_train)\n","        precision_list_epoch_test[epoch] = np.mean(precision_list_batch_test)\n","        recall_list_epoch_test[epoch] = np.mean(recall_list_batch_test)\n","        f1_list_epoch_test[epoch] = np.mean(f1_list_batch_test)\n","        accuracy_list_epoch_test[epoch] = np.mean(accuracy_list_batch_test)\n","\n","        print (f'Train - Época: {epoch+1} -- Generator Loss: {np.mean(g_loss_list_batch_train_aux)}, Discriminator Loss: {np.mean(d_loss_list_batch_train_aux)}')\n","        print (f'Test - Época: {epoch+1} -- Generator Loss: {np.mean(g_loss_list_batch_test_aux)}, Discriminator Loss: {np.mean(d_loss_list_batch_test_aux)}\\n')\n","        print (f'Tomó {time.time()-start} segundos. \\n\\n')\n","\n","        if epoch % 2 == 0:\n","            show_samples(4, noise_dim, g_model, epoch)\n","        if epoch % 2 == 0:\n","            g_model.save(f\"/content/drive/MyDrive/Lembo/gmodel_m5_cifar10_{epoch}.h5\")\n","            d_model.save(f\"/content/drive/MyDrive/Lembo/dmodel_m5_cifar10_{epoch}.h5\")\n","            epochs = np.arange(1, epoch_count+1)\n","            np.savez(f\"/content/drive/MyDrive/Lembo/metricas_m5_cifar10_{epoch}.npz\", epochs= epochs, d_loss_itern_train=d_loss_list_itern_train, g_loss_list_itern_train=g_loss_list_itern_train, d_loss_list_itern_test=d_loss_list_itern_test, g_loss_list_itern_test=g_loss_list_itern_test, d_loss_list_epoch_train=d_loss_list_epoch_train, g_loss_list_epoch_train=g_loss_list_epoch_train, d_loss_list_epoch_test=d_loss_list_epoch_test, g_loss_list_epoch_test=g_loss_list_epoch_test, precision_list_epoch_train=precision_list_epoch_train, recall_list_epoch_train=recall_list_epoch_train, f1_list_epoch_train=f1_list_epoch_train, accuracy_list_epoch_train=accuracy_list_epoch_train, precision_list_epoch_test=precision_list_epoch_test, recall_list_epoch_test=recall_list_epoch_test, f1_list_epoch_test=f1_list_epoch_test, accuracy_list_epoch_test=accuracy_list_epoch_test)\n","\n","    return d_loss_list_itern_train, g_loss_list_itern_train, d_loss_list_itern_test, g_loss_list_itern_test, d_loss_list_epoch_train, g_loss_list_epoch_train, d_loss_list_epoch_test, g_loss_list_epoch_test, precision_list_epoch_train, recall_list_epoch_train, f1_list_epoch_train, accuracy_list_epoch_train, precision_list_epoch_test, recall_list_epoch_test, f1_list_epoch_test, accuracy_list_epoch_test"]},{"cell_type":"markdown","metadata":{"id":"P3TKHdJrHjDV"},"source":["- Entrenamiento del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3263369,"status":"ok","timestamp":1718848200635,"user":{"displayName":"Juan Pablo Morales","userId":"00326788642118637469"},"user_tz":180},"id":"YICmmR10HjDV","outputId":"b4504eda-b04b-4b99-a91d-abd58e9256f6"},"outputs":[],"source":["train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)) # Se crea un dataset con los datos de entrenamiento\n","train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size) # Se mezclan los datos del dataset cada 1000 y se agrupan en batches de a 64\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)) # Se crea un dataset con los datos de test\n","test_dataset = test_dataset.shuffle(buffer_size=1000).batch(batch_size) # Se mezclan los datos del dataset cada 1000 y se agrupan en batches de a 64\n","\n","d_loss_list_itern_train, g_loss_list_itern_train, d_loss_list_itern_test, g_loss_list_itern_test, d_loss_list_epoch_train, g_loss_list_epoch_train, d_loss_list_epoch_test, g_loss_list_epoch_test, precision_list_epoch_train, recall_list_epoch_train, f1_list_epoch_train, accuracy_list_epoch_train, precision_list_epoch_test, recall_list_epoch_test, f1_list_epoch_test, accuracy_list_epoch_test = train(train_dataset, test_dataset, epoch_count, batch_size)\n","epochs = np.arange(1, epoch_count+1)\n","\n","np.savez(\"/content/drive/MyDrive/Lembo/metricas_m5_cifar10.npz\", epochs= epochs, d_loss_itern_train=d_loss_list_itern_train, g_loss_list_itern_train=g_loss_list_itern_train, d_loss_list_itern_test=d_loss_list_itern_test, g_loss_list_itern_test=g_loss_list_itern_test, d_loss_list_epoch_train=d_loss_list_epoch_train, g_loss_list_epoch_train=g_loss_list_epoch_train, d_loss_list_epoch_test=d_loss_list_epoch_test, g_loss_list_epoch_test=g_loss_list_epoch_test, precision_list_epoch_train=precision_list_epoch_train, recall_list_epoch_train=recall_list_epoch_train, f1_list_epoch_train=f1_list_epoch_train, accuracy_list_epoch_train=accuracy_list_epoch_train, precision_list_epoch_test=precision_list_epoch_test, recall_list_epoch_test=recall_list_epoch_test, f1_list_epoch_test=f1_list_epoch_test, accuracy_list_epoch_test=accuracy_list_epoch_test) \n","g_model.save(\"/content/drive/MyDrive/Lembo/gmodel_m5_cifar10.keras\")\n","d_model.save(\"/content/drive/MyDrive/Lembo/dmodel_m5_cifar10.keras\")\n","g_model.save(\"/content/drive/MyDrive/Lembo/gmodel_m5_cifar10.h5\")\n","d_model.save(\"/content/drive/MyDrive/Lembo/dmodel_m5_cifar10.h5\")\n","g_model.save(\"/content/drive/MyDrive/Lembo/gmodel_m5_cifar10.tf\")\n","d_model.save(\"/content/drive/MyDrive/Lembo/dmodel_m5_cifar10.tf\")"]},{"cell_type":"markdown","metadata":{"id":"XCVM5UvSHjDV"},"source":["# Graficas de la pérdida para el generador y el discriminador en función del número de iteraciones"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":7853,"status":"ok","timestamp":1718848208479,"user":{"displayName":"Juan Pablo Morales","userId":"00326788642118637469"},"user_tz":180},"id":"sjqAnnfOHjDW","outputId":"c53ff682-8f95-4b01-9853-a7172f6ed0e3"},"outputs":[],"source":["data = np.load(\"/content/drive/MyDrive/Lembo/metricas_m5_cifar10.npz\")\n","\n","iteration_train = np.arange(1, len(X_train)/batch_size*epoch_count+1)\n","iteration_test = np.arange(1, len(X_test)/batch_size*epoch_count+1)\n","d_loss_itern_train = data['d_loss_itern_train']\n","g_loss_itern_train = data['g_loss_itern_train']\n","d_loss_itern_test = data['d_loss_itern_test']\n","g_loss_itern_test = data['g_loss_itern_test']\n","\n","fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n","axs[0].plot(iteration_train, g_loss_itern_train, label='Generator Loss - train')\n","axs[0].plot(iteration_train, d_loss_itern_train, label='Discriminator Loss - train')\n","axs[0].set_xlabel('Iteración')\n","axs[0].set_ylabel('Loss')\n","axs[0].legend()\n","axs[1].plot(iteration_test, g_loss_itern_test, label='Generator Loss - test')\n","axs[1].plot(iteration_test, d_loss_itern_test, label='Discriminator Loss - test')\n","axs[1].set_xlabel('Iteración')\n","axs[1].set_ylabel('Loss')\n","axs[1].legend()\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BBbUtZHOHjDW"},"source":["# Graficas de la pérdida para el generador y el discriminador en función de las épocas"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":544},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1718848208479,"user":{"displayName":"Juan Pablo Morales","userId":"00326788642118637469"},"user_tz":180},"id":"u6QnKH6wHjDW","outputId":"92113133-dfb5-4d20-f1a3-b2cca99ee295"},"outputs":[],"source":["data = np.load(\"/content/drive/MyDrive/Lembo/metricas_m5_cifar10.npz\")\n","\n","epochs = data['epochs']\n","d_loss_train = data['d_loss_train']\n","g_loss_train = data['g_loss_train']\n","d_loss_test = data['d_loss_test']\n","g_loss_test = data['g_loss_test']\n","\n","plt.figure(figsize=(8,6))\n","plt.plot(epochs, g_loss_train, label='Generator Loss - train')\n","plt.plot(epochs, d_loss_train, label='Discriminator Loss - train')\n","plt.plot(epochs, g_loss_test, label='Generator Loss - test')\n","plt.plot(epochs, d_loss_test, label='Discriminator Loss - test')\n","plt.xlabel('Épocas')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Graficas de las métricas para el discriminador en función del número de épocas"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1028,"status":"ok","timestamp":1718848209503,"user":{"displayName":"Juan Pablo Morales","userId":"00326788642118637469"},"user_tz":180},"id":"JzZ_mdFJHjDW","outputId":"58f066a5-ad80-4e0b-9db9-48a73e67c570"},"outputs":[],"source":["data = np.load(\"/content/drive/MyDrive/Lembo/metricas_m5_cifar10.npz\")\n","epochs = data['epochs']\n","precision_train = data['precision_train']\n","recall_train = data['recall_train']\n","f1_train = data['f1_train']\n","accuracy_train = data['accuracy_train']\n","precision_test = data['precision_test']\n","recall_test = data['recall_test']\n","f1_test = data['f1_test']\n","accuracy_test = data['accuracy_test']\n","\n","fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n","epochs = np.arange(1, epoch_count + 1)\n","axs[0, 0].plot(epochs, precision_train, label='Train')\n","axs[0, 0].plot(epochs, precision_test, label='Test')\n","axs[0, 0].set_xlabel('Épocas')\n","axs[0, 0].set_ylabel('Precisión')\n","axs[0, 0].legend()\n","axs[0, 0].set_title('Precisión')\n","axs[0, 1].plot(epochs, recall_train, label='Train')\n","axs[0, 1].plot(epochs, recall_test, label='Test')\n","axs[0, 1].set_xlabel('Épocas')\n","axs[0, 1].set_ylabel('Recall')\n","axs[0, 1].legend()\n","axs[0, 1].set_title('Recall')\n","axs[1, 0].plot(epochs, f1_train, label='Train')\n","axs[1, 0].plot(epochs, f1_test, label='Test')\n","axs[1, 0].set_xlabel('Épocas')\n","axs[1, 0].set_ylabel('F1-score')\n","axs[1, 0].legend()\n","axs[1, 0].set_title('F1-score')\n","axs[1, 1].plot(epochs, accuracy_train, label='Train')\n","axs[1, 1].plot(epochs, accuracy_test, label='Test')\n","axs[1, 1].set_xlabel('Épocas')\n","axs[1, 1].set_ylabel('Accuracy')\n","axs[1, 1].legend()\n","axs[1, 1].set_title('Accuracy')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ysff7mPxHjDW"},"source":["# Generación de una imagen del cifar 100 pedida por el usuario"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"elapsed":396,"status":"error","timestamp":1718848743727,"user":{"displayName":"Ignacio Lembo Ferrari","userId":"11139872009396123551"},"user_tz":180},"id":"YQuOYaPnHjDW","outputId":"41334dde-d08b-41b4-f188-dbe47262b6dc"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","import matplotlib.pyplot as plt\n","from keras.preprocessing import image\n","\n","noise_dim = 100 # Dimension del ruido\n","\n","tags = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n","\n","def get_index(word, tags_list):\n","    try:\n","        return tags_list.index(word)\n","    except ValueError:\n","        return -1  # Devuelve -1 si la palabra no se encuentra en la lista\n","    \n","# Cargar el modelo generador\n","g_model = load_model('/content/drive/MyDrive/Lembo/gmodel_m5_cifar10.h5')\n","\n","imagen_a_generar = 'apple'\n","index = get_index(imagen_a_generar, tags)\n","\n","#label = tf.expand_dims(numero_a_generar, axis=-1) #expando la dimension de y_train para que quede analogo al ejemplo del cifar10\n","label = tf.ones(1)*index\n","\n","# Generar ruido aleatorio\n","noise = tf.random.normal(shape=(1, noise_dim))\n","\n","# Generar imagen falsa\n","generated_image = g_model([noise, label]) #  Genero imagenes falsas\n","#generated_image = g_model.predict([noise, label])\n","print(\"Tamaño imagen generada: \", generated_image.shape)\n","\n","plt.figure(figsize=(2,2))\n","img = image.array_to_img(generated_image[0], scale=True)\n","plt.imshow(img)\n","plt.axis('off')\n","plt.title(f\"{tags[index]}\")\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1yWSUNLvXwq9ULRSW_21CrJNjaRC-j8tq","timestamp":1718848316134}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
